{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:34:50.248100: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-26 14:34:50.248128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-26 14:34:50.248934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-26 14:34:50.254165: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:absl:Generating dataset mnist (/home/tf/tensorflow_datasets/mnist/3.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/tf/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:35:00.959907: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"ABORTED: All 10 retry attempts failed. The last failure: Error executing an HTTP request: HTTP response code 308\".\n",
      "INFO:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
      "local data directory. If you'd instead prefer to read directly from our public\n",
      "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
      "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2becf5f5cc64949a9e55bab396df0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/tf/tensorflow_datasets/mnist/3.0.1.incompleteWO31AP\n",
      "INFO:absl:Fields info.[citation, splits, supervised_keys, module_name, file_format] from disk and from code do not match. Keeping the one from code.\n",
      "2025-07-26 14:35:02.348306: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split ['train', 'test'], from /home/tf/tensorflow_datasets/mnist/3.0.1\n",
      "2025-07-26 14:35:02.348650: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-26 14:35:02.413319: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-26 14:35:02.413634: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-26 14:35:02.413863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-26 14:35:02.414087: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /home/tf/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:105\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43mtype_spec_from_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:514\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    511\u001b[0m     logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a `TypeSpec` for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    515\u001b[0m     element,\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for ['/home/tf/tensorflow_datasets/mnist/3.0.1/mnist-train.tfrecord-00000-of-00001'] with type list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m OUTPUT_PATH \u001b[38;5;241m=\u001b[39m BASE_PATH\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_simple_01/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmkdir -p \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m OUTPUT_PATH)\n\u001b[0;32m---> 19\u001b[0m (train_ds, test_ds), ds_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_ds)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_ds\u001b[38;5;241m.\u001b[39mcardinality()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:629\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    626\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle_files\u001b[39m\u001b[38;5;124m'\u001b[39m, shuffle_files)\n\u001b[1;32m    627\u001b[0m as_dataset_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_config\u001b[39m\u001b[38;5;124m'\u001b[39m, read_config)\n\u001b[0;32m--> 629\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mas_dataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_info:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ds, dbuilder\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:781\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[1;32m    773\u001b[0m build_single_dataset \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_single_dataset,\n\u001b[1;32m    775\u001b[0m     shuffle_files\u001b[38;5;241m=\u001b[39mshuffle_files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m     as_supervised\u001b[38;5;241m=\u001b[39mas_supervised,\n\u001b[1;32m    780\u001b[0m )\n\u001b[0;32m--> 781\u001b[0m all_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_single_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_ds\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:799\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    796\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mtotal_num_examples \u001b[38;5;129;01mor\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmaxsize\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Auto-cache small datasets which are small enough to fit in memory.\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_cache_ds(\n\u001b[1;32m    807\u001b[0m     split\u001b[38;5;241m=\u001b[39msplit, shuffle_files\u001b[38;5;241m=\u001b[39mshuffle_files, read_config\u001b[38;5;241m=\u001b[39mread_config\n\u001b[1;32m    808\u001b[0m ):\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1252\u001b[0m, in \u001b[0;36mFileReaderBuilder._as_dataset\u001b[0;34m(self, split, decoders, read_config, shuffle_files)\u001b[0m\n\u001b[1;32m   1246\u001b[0m reader \u001b[38;5;241m=\u001b[39m reader_lib\u001b[38;5;241m.\u001b[39mReader(\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir,\n\u001b[1;32m   1248\u001b[0m     example_specs\u001b[38;5;241m=\u001b[39mexample_specs,\n\u001b[1;32m   1249\u001b[0m     file_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfile_format,\n\u001b[1;32m   1250\u001b[0m )\n\u001b[1;32m   1251\u001b[0m decode_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(features\u001b[38;5;241m.\u001b[39mdecode_example, decoders\u001b[38;5;241m=\u001b[39mdecoders)\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:413\u001b[0m, in \u001b[0;36mReader.read\u001b[0;34m(self, instructions, split_infos, read_config, shuffle_files, disable_shuffling, decode_fn)\u001b[0m\n\u001b[1;32m    404\u001b[0m   file_instructions \u001b[38;5;241m=\u001b[39m splits_dict[instruction]\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[1;32m    405\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_files(\n\u001b[1;32m    406\u001b[0m       file_instructions,\n\u001b[1;32m    407\u001b[0m       read_config\u001b[38;5;241m=\u001b[39mread_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m       decode_fn\u001b[38;5;241m=\u001b[39mdecode_fn,\n\u001b[1;32m    411\u001b[0m   )\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_read_instruction_to_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:405\u001b[0m, in \u001b[0;36mReader.read.<locals>._read_instruction_to_ds\u001b[0;34m(instruction)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_instruction_to_ds\u001b[39m(instruction):\n\u001b[1;32m    404\u001b[0m   file_instructions \u001b[38;5;241m=\u001b[39m splits_dict[instruction]\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[0;32m--> 405\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfile_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m      \u001b[49m\u001b[43mread_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdecode_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:445\u001b[0m, in \u001b[0;36mReader.read_files\u001b[0;34m(self, file_instructions, read_config, shuffle_files, disable_shuffling, decode_fn)\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Read serialized example (eventually with `tfds_id`)\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43m_read_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Parse and decode\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_and_decode\u001b[39m(ex: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TreeDict[Tensor]:\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;66;03m# TODO(pierrot): `parse_example` uses\u001b[39;00m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;66;03m# `tf.io.parse_single_example`. It might be faster to use `parse_example`,\u001b[39;00m\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;66;03m# after batching.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m   \u001b[38;5;66;03m# https://www.tensorflow.org/api_docs/python/tf/io/parse_example\u001b[39;00m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow_datasets/core/reader.py:254\u001b[0m, in \u001b[0;36m_read_files\u001b[0;34m(file_instructions, read_config, shuffle_files, disable_shuffling, file_format)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_shuffling:\n\u001b[1;32m    247\u001b[0m   _verify_read_config_for_ordered_dataset(\n\u001b[1;32m    248\u001b[0m       read_config,\n\u001b[1;32m    249\u001b[0m       interleave_cycle_length\u001b[38;5;241m=\u001b[39mcycle_length,\n\u001b[1;32m    250\u001b[0m       shuffle_files\u001b[38;5;241m=\u001b[39mshuffle_files,\n\u001b[1;32m    251\u001b[0m   )\n\u001b[0;32m--> 254\u001b[0m instruction_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# On distributed environments, we can shard per-file if a\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# `tf.distribute.InputContext` object is provided (e.g. from\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# `experimental_distribute_datasets_from_function`)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_config\u001b[38;5;241m.\u001b[39minput_context:\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:825\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:110\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    105\u001b[0m     spec \u001b[38;5;241m=\u001b[39m type_spec_from_value(t, use_fallback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 110\u001b[0m       \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;66;03m# To avoid a circular dependency between dataset_ops and structure,\u001b[39;00m\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;66;03m# we check the class name instead of using `isinstance`.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetSpec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:696\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    695\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:335\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    174\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:284\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    283\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:296\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    293\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m--> 102\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/eager/context.py:588\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m opts \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_NewContextOptions()\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m   config_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m    589\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetConfig(opts, config_str)\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1134\u001b[0m, in \u001b[0;36mContext.config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the ConfigProto with all runtime deltas applied.\"\"\"\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# Ensure physical devices have been discovered and config has been imported\u001b[39;00m\n\u001b[0;32m-> 1134\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_physical_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m config \u001b[38;5;241m=\u001b[39m config_pb2\u001b[38;5;241m.\u001b[39mConfigProto()\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1564\u001b[0m, in \u001b[0;36mContext._initialize_physical_devices\u001b[0;34m(self, reinitialize)\u001b[0m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reinitialize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1562\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1564\u001b[0m devs \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_ListPhysicalDevices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1566\u001b[0m     PhysicalDevice(name\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode(), device_type\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1567\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devs\n\u001b[1;32m   1568\u001b[0m ]\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_device_to_index \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1570\u001b[0m     p: i \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physical_devices)\n\u001b[1;32m   1571\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/datasets/keras_example\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, os\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "BASE_PATH = \"../../../../../local_data/practice/tfds/\"\n",
    "DATA_PATH = \"../../../../../local_data/tfds/\"\n",
    "OUTPUT_PATH = BASE_PATH+\"mnist_simple_01/\"\n",
    "os.system(\"mkdir -p \" + OUTPUT_PATH)\n",
    "\n",
    "\n",
    "(train_ds, test_ds), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    data_dir=DATA_PATH,\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "print(test_ds)\n",
    "print(f\"Number of training samples: {train_ds.cardinality()}\")\n",
    "print(f\"Number of test samples: {test_ds.cardinality()}\")\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(int(label))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe65753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4\n",
      "1    1\n",
      "2    0\n",
      "3    7\n",
      "4    8\n",
      "5    1\n",
      "6    2\n",
      "7    7\n",
      "8    1\n",
      "9    6\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 21:29:34.899715: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "c1=tfds.as_dataframe(train_ds.take(10), ds_info)\n",
    "col1 = c1['label']\n",
    "# col1 = pd.DataFrame(train_ds, columns=[\"image\",\"train_ds\"])\n",
    "print(col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92facee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83dc02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.cache()\n",
    "# train_ds = train_ds.shuffle(ds_info.splits['train'].num_examples)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e129a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "test_ds = test_ds.cache()\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eeb7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750/3750 [==============================] - 6s 2ms/step - loss: 0.3957 - sparse_categorical_accuracy: 0.8884 - val_loss: 0.2561 - val_sparse_categorical_accuracy: 0.9271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7af91c0da4a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(batch_size, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=1,\n",
    "    validation_data=test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab408f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750,)\n",
      "3750/3750 [==============================] - 3s 670us/step\n",
      "                                                  label        p0         p1  \\\n",
      "0      [4, 1, 0, 7, 8, 1, 2, 7, 1, 6, 6, 4, 7, 7, 3, 3] -7.974104  -1.874438   \n",
      "1      [7, 9, 9, 1, 0, 6, 6, 9, 9, 4, 8, 9, 4, 7, 3, 3] -7.499573   4.179980   \n",
      "2      [0, 9, 4, 9, 0, 6, 8, 4, 7, 2, 6, 0, 3, 1, 1, 7]  6.075268 -10.879297   \n",
      "3      [2, 4, 4, 6, 5, 1, 9, 3, 2, 4, 3, 4, 4, 7, 5, 8] -3.154041 -13.878533   \n",
      "4      [1, 1, 4, 1, 5, 3, 5, 8, 4, 1, 1, 4, 5, 3, 2, 4] -2.916331  -8.122678   \n",
      "...                                                 ...       ...        ...   \n",
      "59995                                               NaN -7.877224 -12.248287   \n",
      "59996                                               NaN -9.165756   4.454875   \n",
      "59997                                               NaN  0.808035 -11.091747   \n",
      "59998                                               NaN -8.930107   5.770471   \n",
      "59999                                               NaN -1.994019  -9.150037   \n",
      "\n",
      "              p2        p3        p4        p5         p6        p7        p8  \\\n",
      "0      -5.273805 -4.320768  2.867625 -1.910296  -3.202208 -4.827733 -1.829898   \n",
      "1      -0.489544 -1.512232 -3.206689 -5.826322  -7.341101  0.522837 -1.525539   \n",
      "2      -2.485017 -4.591109 -6.118352 -0.547507  -0.456059 -9.184472 -3.823445   \n",
      "3      -3.756330 -5.254288 -3.901113 -6.598130 -13.759611  7.796967 -4.606932   \n",
      "4      -3.229439  0.186140 -7.288214 -1.350189  -7.740355 -7.406330  3.721932   \n",
      "...          ...       ...       ...       ...        ...       ...       ...   \n",
      "59995  -3.542810 -2.456888 -6.832332 -8.423465 -17.936378  9.128709 -3.723532   \n",
      "59996   0.713926 -1.059478 -2.504832 -7.305923  -9.681799  0.151455 -1.290978   \n",
      "59997  -0.823555 -4.643055 -3.505729  0.871068   5.821482 -8.144022 -4.670308   \n",
      "59998  -2.029844  0.075598 -1.987956 -1.968403  -3.813451 -3.635951 -0.635591   \n",
      "59999 -15.173927 -5.271510  3.092366  7.352747  -7.096550 -4.884930  2.183368   \n",
      "\n",
      "             p9  \n",
      "0     -1.067558  \n",
      "1     -2.542782  \n",
      "2     -4.921444  \n",
      "3     -0.971665  \n",
      "4     -5.105397  \n",
      "...         ...  \n",
      "59995 -0.262629  \n",
      "59996 -3.276188  \n",
      "59997 -7.392466  \n",
      "59998 -1.652932  \n",
      "59999 -2.828174  \n",
      "\n",
      "[60000 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "c1=tfds.as_dataframe(train_ds.take(5000), ds_info)\n",
    "col1 = c1['label']\n",
    "print(col1.shape)\n",
    "\n",
    "pred = model.predict(train_ds)\n",
    "# print(\"pred\\n\",pred)\n",
    "# col2 = pd.DataFrame(pred[0:20], columns=[\"p0\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\",\"p6\",\"p7\",\"p8\",\"p9\"])\n",
    "col2 = pd.DataFrame(pred, columns=[\"p0\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\",\"p6\",\"p7\",\"p8\",\"p9\"])\n",
    "# print(col2)\n",
    "\n",
    "compare = pd.concat([col1, col2], axis=1)\n",
    "print(compare)\n",
    "compare.to_csv(OUTPUT_PATH + \"pred_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e687067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 867us/step\n",
      "                                                 label         p0         p1  \\\n",
      "0     [4, 1, 0, 7, 8, 1, 2, 7, 1, 6, 6, 4, 7, 7, 3, 3]  -3.265374  -4.515430   \n",
      "1     [7, 9, 9, 1, 0, 6, 6, 9, 9, 4, 8, 9, 4, 7, 3, 3]  10.253217 -17.563599   \n",
      "2     [0, 9, 4, 9, 0, 6, 8, 4, 7, 2, 6, 0, 3, 1, 1, 7]  -7.998972 -12.060715   \n",
      "3     [2, 4, 4, 6, 5, 1, 9, 3, 2, 4, 3, 4, 4, 7, 5, 8]  -4.729486   0.177743   \n",
      "4     [1, 1, 4, 1, 5, 3, 5, 8, 4, 1, 1, 4, 5, 3, 2, 4]  -4.226891 -12.413978   \n",
      "...                                                ...        ...        ...   \n",
      "9995                                               NaN  -1.599782  -7.026804   \n",
      "9996                                               NaN  -9.002808  -8.298120   \n",
      "9997                                               NaN  -4.201391  -8.996775   \n",
      "9998                                               NaN   9.253925 -12.211276   \n",
      "9999                                               NaN  -2.396875  -6.491379   \n",
      "\n",
      "            p2         p3        p4        p5         p6         p7        p8  \\\n",
      "0     4.591537  -1.174222 -4.993389 -3.067190  -6.298978  -8.205939 -4.971118   \n",
      "1    -1.198133 -10.301422 -8.405008  0.973717  -5.263607 -12.284986 -0.964807   \n",
      "2    -4.870873  -4.549546  4.958075 -8.436917  -6.745897   1.150829 -5.205309   \n",
      "3    -2.313173  -0.508000 -5.012874 -1.654255  -4.164176  -6.590121  3.600281   \n",
      "4    -3.061710  -3.171891 -3.951161 -6.993460 -14.772624   8.587796 -4.304127   \n",
      "...        ...        ...       ...       ...        ...        ...       ...   \n",
      "9995 -1.766222  -1.611026 -5.285413  4.414586  -0.011100 -10.379722  1.594089   \n",
      "9996 -8.046137   0.604056 -1.526220 -0.027025  -8.900262  -0.486503 -2.110397   \n",
      "9997 -3.099261  -4.357946 -4.356802 -1.190159  -0.373092 -15.163993  5.143875   \n",
      "9998 -3.235155  -3.673941 -6.485071 -0.711792  -7.125770  -6.516787 -3.303845   \n",
      "9999 -6.018752  -3.688976 -2.142224  7.147227  -3.094228  -9.343389  2.528220   \n",
      "\n",
      "            p9  \n",
      "0    -5.062407  \n",
      "1    -8.252603  \n",
      "2    -0.130314  \n",
      "3    -1.809015  \n",
      "4     0.110653  \n",
      "...        ...  \n",
      "9995 -4.586029  \n",
      "9996  3.949026  \n",
      "9997 -6.046017  \n",
      "9998 -5.720924  \n",
      "9999 -2.823527  \n",
      "\n",
      "[10000 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_ds)\n",
    "# print(\"pred\\n\",pred)\n",
    "# col2 = pd.DataFrame(pred[0:20], columns=[\"p0\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\",\"p6\",\"p7\",\"p8\",\"p9\"])\n",
    "col2 = pd.DataFrame(pred, columns=[\"p0\",\"p1\",\"p2\",\"p3\",\"p4\",\"p5\",\"p6\",\"p7\",\"p8\",\"p9\"])\n",
    "# print(col2)\n",
    "\n",
    "compare = pd.concat([col1, col2], axis=1)\n",
    "print(compare)\n",
    "compare.to_csv(OUTPUT_PATH + \"pred_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jh_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
