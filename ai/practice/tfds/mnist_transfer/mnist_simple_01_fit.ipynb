{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/tf/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/tf/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split ['train', 'test'], from /home/tf/tensorflow_datasets/mnist/3.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds: <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
      "Number of training samples: 60000\n",
      "Number of test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/datasets/keras_example\n",
    "# without transfer learning\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "BASE_PATH = \"../../../../../local_data/practice/tfds/\"\n",
    "DATA_PATH = \"../../../../../local_data/tfds/\"\n",
    "OUTPUT_PATH = BASE_PATH+\"mnist_simple_01/\"\n",
    "os.system(\"mkdir -p \" + OUTPUT_PATH)\n",
    "\n",
    "\n",
    "(train_ds, test_ds), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "print(\"train_ds:\",train_ds)\n",
    "print(f\"Number of training samples: {train_ds.cardinality()}\")\n",
    "print(f\"Number of test samples: {test_ds.cardinality()}\")\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(int(label))\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fe65753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     4\n",
      "1     1\n",
      "2     0\n",
      "3     7\n",
      "4     8\n",
      "5     1\n",
      "6     2\n",
      "7     7\n",
      "8     1\n",
      "9     6\n",
      "10    6\n",
      "11    4\n",
      "12    7\n",
      "13    7\n",
      "14    3\n",
      "15    3\n",
      "16    7\n",
      "17    9\n",
      "18    9\n",
      "19    1\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 21:47:50.564319: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "c1=tfds.as_dataframe(train_ds.take(20), ds_info)\n",
    "col1 = c1['label']\n",
    "# col1 = pd.DataFrame(train_ds, columns=[\"image\",\"train_ds\"])\n",
    "print(col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b92facee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83dc02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.cache()\n",
    "# train_ds = train_ds.shuffle(ds_info.splits['train'].num_examples)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e129a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "test_ds = test_ds.cache()\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eeb7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                12560     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12730 (49.73 KB)\n",
      "Trainable params: 12730 (49.73 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "3750/3750 [==============================] - 12s 3ms/step - loss: 0.3826 - sparse_categorical_accuracy: 0.8916 - val_loss: 0.2443 - val_sparse_categorical_accuracy: 0.9297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x79d0cc7f06a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(batch_size, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "model.summary()\n",
    "epochs = 1\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ba73151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to  epochs_1.000_date_20250707-214802.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/miniconda3/envs/jh_class/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# filename = f\"acc_{score:.3f}_date_{timestr}.h5\"\n",
    "filename = f\"epochs_{epochs:.3f}_date_{timestr}.h5\"\n",
    "fullpath = f\"{OUTPUT_PATH}{filename}\"\n",
    "print(\"Saving model to \", filename)\n",
    "model.save(fullpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jh_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
