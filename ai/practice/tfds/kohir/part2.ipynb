{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with TensorFlow Datasets -part 2; Intro to tfds and its methods\n",
    "\n",
    "https://kvirajdatt.medium.com/starting-with-tensorflow-datasets-part-2-intro-to-tfds-and-its-methods-32d3ac36420f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../../local_data/practice/tfds/kohir/\n",
      "../../../../../local_data/tfds/\n",
      "../../../../../local_data/practice/tfds/kohir/part2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import logging, os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "BASE_PATH = \"../../../../../local_data/practice/tfds/kohir/\"\n",
    "DATA_PATH = \"../../../../../local_data/tfds/\"\n",
    "OUTPUT_PATH = BASE_PATH+\"part2\"\n",
    "os.system(\"mkdir -p \" + OUTPUT_PATH)\n",
    "\n",
    "print(BASE_PATH)\n",
    "print(DATA_PATH)\n",
    "print(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ../../../../../local_data/tfds/mnist/3.0.1\n",
      "INFO:absl:Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (../../../../../local_data/tfds/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split train, from ../../../../../local_data/tfds/mnist/3.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Number of training samples: 60000\n",
      "tfds.core.DatasetInfo(\n",
      "    name='mnist',\n",
      "    full_name='mnist/3.0.1',\n",
      "    description=\"\"\"\n",
      "    The MNIST database of handwritten digits.\n",
      "    \"\"\",\n",
      "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
      "    data_path='../../../../../local_data/tfds/mnist/3.0.1',\n",
      "    file_format=tfrecord,\n",
      "    download_size=11.06 MiB,\n",
      "    dataset_size=21.00 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@article{lecun2010mnist,\n",
      "      title={MNIST handwritten digit database},\n",
      "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
      "      volume={2},\n",
      "      year={2010}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ds,info = tfds.load(\n",
    "        'mnist',\n",
    "        data_dir=DATA_PATH, \n",
    "        split='train', \n",
    "        shuffle_files=True,\n",
    "        with_info=True\n",
    ")\n",
    "print(ds)\n",
    "\n",
    "# #trainset, validationset, test-set, metadata\n",
    "# (train_ds, val_ds, test_ds), metadata = tfds.load('tf_flowers',\n",
    "#                 data_dir=DATA_PATH,\n",
    "#                 split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "#                 with_info=True,\n",
    "#                 as_supervised=True,\n",
    "# )\n",
    "print(f\"Number of training samples: {ds.cardinality()}\")\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 14:53:53.129587: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "ds = ds.take(1)\n",
    "\n",
    "for image, label in ds:  # example is (image, label)\n",
    "  # print(image.shape, label)  \n",
    "  print(image, label)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "t81_558_class_01_1_overview.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jh_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
