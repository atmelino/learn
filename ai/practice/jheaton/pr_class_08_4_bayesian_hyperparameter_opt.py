# Ignore useless W0819 warnings generated by TensorFlow 2.0.
# Hopefully can remove this ignore in the future.
# See https://github.com/tensorflow/tensorflow/issues/31308
import logging, os
import pandas as pd
import os
import numpy as np
import time
import tensorflow.keras
import statistics
import tensorflow.keras
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import ShuffleSplit
from tensorflow.keras.layers import LeakyReLU, PReLU
from tensorflow.keras.optimizers import Adam
from scipy.stats import zscore
from bayes_opt import BayesianOptimization
import time
import warnings  # Supress NaN warnings

BASE_PATH = "../../../../local_data/practice/jheaton"
OUTPUT_PATH = os.path.join(BASE_PATH, "pr_class_08_4_bayesian_hyperparameter_opt/")
os.system("mkdir -p " + OUTPUT_PATH)

logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
warnings.filterwarnings("ignore", category=RuntimeWarning)

# Read the data set
df = pd.read_csv(
    "https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv",
    na_values=["NA", "?"],
)
# Generate dummies for job
df = pd.concat([df, pd.get_dummies(df["job"], prefix="job")], axis=1)
df.drop("job", axis=1, inplace=True)
# Generate dummies for area
df = pd.concat([df, pd.get_dummies(df["area"], prefix="area")], axis=1)
df.drop("area", axis=1, inplace=True)
# Missing values for income
med = df["income"].median()
df["income"] = df["income"].fillna(med)
# Standardize ranges
df["income"] = zscore(df["income"])
df["aspect"] = zscore(df["aspect"])
df["save_rate"] = zscore(df["save_rate"])
df["age"] = zscore(df["age"])
df["subscriptions"] = zscore(df["subscriptions"])
# Convert to numpy - Classification
x_columns = df.columns.drop("product").drop("id")
x = df[x_columns].values
# print("x\n", x)
# print("x.shape=",x.shape)


dummies = pd.get_dummies(df["product"])  # Classification
products = dummies.columns
y = dummies.values
# print("y\n", y)


def generate_model(dropout, neuronPct, neuronShrink):
    # We start with some percent of 5000 starting neurons on
    # the first hidden layer.
    neuronCount = int(neuronPct * 5000)
    # print("neuronCount=",neuronCount)
    # print("x.shape[1]=",x.shape[1])

    # Construct neural network
    model = Sequential()
    # So long as there would have been at least 25 neurons and
    # fewer than 10
    # layers, create a new layer.
    layer = 0
    while neuronCount > 25 and layer < 10:
        # The first (0th) layer needs an input input_dim(neuronCount)
        if layer == 0:
            # print("neuronCount=",neuronCount)
            model.add(Dense(neuronCount, input_dim=x.shape[1], activation=PReLU()))
        else:
            model.add(Dense(neuronCount, activation=PReLU()))
            layer += 1
        # Add dropout after each hidden layer
        model.add(Dropout(dropout))
        # Shrink neuron count for each layer
        neuronCount = round(neuronCount * neuronShrink)
        # neuronCount = neuronCount * neuronShrink
    model.add(Dense(y.shape[1], activation="softmax"))  # Output
    return model


# Generate a model and see what the resulting structure looks like.
# model = generate_model(dropout=0.2, neuronPct=0.1, neuronShrink=0.25)
# model = generate_model(dropout=0.2, neuronPct=0.9, neuronShrink=0.985)
# model.summary()
# exit()

SPLITS = 2
EPOCHS = 500
PATIENCE = 10
TRAIN = True
call_count = 0

def evaluate_network(dropout, learning_rate, neuronPct, neuronShrink):
    global call_count
    call_count += 1
    
    # print(f"call {call_count} evaluate_network: dr={dropout:.4f} lr={learning_rate:.4f} nP={neuronPct:.4f} nS={neuronShrink:.4f}")
    print(f"call {call_count} evaluate_network: dr={dropout} lr={learning_rate} nP={neuronPct} nS={neuronShrink}")

    # Bootstrap
    # for Classification
    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)
    # for Regression
    # boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1)
    # Track progress
    mean_benchmark = []
    epochs_needed = []
    # Loop through samples
    for train, test in boot.split(x, df["product"]):
        start_time = time.time()
        # Split train and test
        # x_train = x[train]
        # y_train = y[train]
        # x_test = x[test]
        # y_test = y[test]
        x_train = np.asarray(x[train]).astype(np.float32)
        y_train = np.asarray(y[train]).astype(np.float32)
        x_test = np.asarray(x[test]).astype(np.float32)
        y_test = np.asarray(y[test]).astype(np.float32)

        model = generate_model(dropout, neuronPct, neuronShrink)
        # model.summary()

        if TRAIN == True:
            model.compile(
                loss="categorical_crossentropy",
                optimizer=Adam(learning_rate=learning_rate),
            )
            monitor = EarlyStopping(
                monitor="val_loss",
                min_delta=1e-3,
                patience=PATIENCE,
                verbose=0,
                mode="auto",
                restore_best_weights=True,
            )
            # Train on the bootstrap sample
            model.fit(
                x_train,
                y_train,
                validation_data=(x_test, y_test),
                callbacks=[monitor],
                verbose=0,
                epochs=EPOCHS,
            )
            epochs = monitor.stopped_epoch
            epochs_needed.append(epochs)
            # Predict on the out of boot (validation)
            pred = model.predict(x_test)
            # print("pred",pred)
            # print("y_test",y_test)
            # Measure this bootstrap's log loss
            y_compare = np.argmax(y_test, axis=1)  # For log loss calculation
            pred_max = np.argmax(pred, axis=1)  # For log loss calculation

            col1 = pd.DataFrame(y_compare, columns=["y_compare"])
            col2 = pd.DataFrame(pred_max, columns=["pred_max"])
            col3 = pd.DataFrame(pred, columns=["p1","p2","p3","p4","p5","p6","p7"])
            compare = pd.concat([col1, col2,col3], axis=1)
            compare.columns = ["y_test", "pred_max","p1","p2","p3","p4","p5","p6","p7"]
            # print(compare)
            compare.to_csv(OUTPUT_PATH + "df_y_pred_iter"+str(call_count)+".csv", index=False)
            score = metrics.accuracy_score(y_compare, pred_max)
            print(f"Fold score (accuracy): {score}")

            score = metrics.log_loss(y_compare, pred)
            mean_benchmark.append(score)
            print("mean_benchmark=", mean_benchmark)
            m1 = statistics.mean(mean_benchmark)
            m2 = statistics.mean(epochs_needed)
            mdev = statistics.pstdev(mean_benchmark)
            # Record this iteration
            time_took = time.time() - start_time
        else:
            m1=.2
        tensorflow.keras.backend.clear_session()
    return -m1

# Nicely formatted time string
def hms_string(sec_elapsed):
    h = int(sec_elapsed / (60 * 60))
    m = int((sec_elapsed % (60 * 60)) / 60)
    s = sec_elapsed % 60
    return f"{h}:{m:>02}:{s:>05.2f}"

# evaluate_network(dropout=0.2, learning_rate=1e-3, neuronPct=0.2, neuronShrink=0.2)
# evaluate_network(dropout=0.11679448002633344, learning_rate=0.044700185567872745, neuronPct=0.5546086159149624, neuronShrink=0.8547725918091494)
# evaluate_network(dropout=0.2, learning_rate=1e-3, neuronPct=0.8, neuronShrink=0.90)
# evaluate_network(dropout=0.2, learning_rate=1e-3, neuronPct=0.8, neuronShrink=0.95)
# evaluate_network(dropout=0.45181097883636695, learning_rate=0.025462793367289474, neuronPct=0.9943774602806296, neuronShrink=0.9881956190726225)
# evaluate_network(dropout=0.2, learning_rate=1e-3, neuronPct=0.8, neuronShrink=0.99)
# exit()

# Bounded region of parameter space
original_pbounds = {
    "dropout": (0.0, 0.499),
    "learning_rate": (0.0, 0.1),
    "neuronPct": (0.01, 1),
    "neuronShrink": (0.01, 1),
}

pbounds_01 = {
    "dropout": (0.0, 0.499),
    "learning_rate": (0.0, 0.1),
    "neuronPct": (0.01, 1),
    "neuronShrink": (0.01, 0.8),
}

# -0.6277	0.07323	0.009234	0.1944	0.3175
# pbounds = {
#     "dropout": (0.03, 0.1),
#     "learning_rate": (0.004, 0.02),
#     "neuronPct": (0.1, 0.25),
#     "neuronShrink": (0.25, 0.35),
# }

# -0.6277	0.07323	0.009234	0.1944	0.3175
pbounds = {
    "dropout": (0.07, 0.075),
    "learning_rate": (0.009, 0.01),
    "neuronPct": (0.18, 0.2),
    "neuronShrink": (0.31, 0.32),
}


optimizer = BayesianOptimization(
    f=evaluate_network,
    pbounds=pbounds,
    # pbounds=original_pbounds,
    verbose=2,  # verbose = 1 prints only when a maximum
    # is observed, verbose = 0 is silent
    random_state=1,
)
start_time = time.time()
optimizer.maximize(
    init_points=5,
    n_iter=2,
)
time_took = time.time() - start_time
print(f"Total runtime: {hms_string(time_took)}")
print(optimizer.max)

for i, res in enumerate(optimizer.res):
    print("Iteration {}: \n\t{}".format(i, res))