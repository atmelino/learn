# Ignore useless W0819 warnings generated by TensorFlow 2.0.
# Hopefully can remove this ignore in the future.
# See https://github.com/tensorflow/tensorflow/issues/31308
import logging, os
import pandas as pd
import os
import numpy as np
import time
import tensorflow.keras
import statistics
import tensorflow.keras
from sklearn import metrics
from sklearn.model_selection import StratifiedKFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import ShuffleSplit
from tensorflow.keras.layers import LeakyReLU, PReLU
from tensorflow.keras.optimizers import Adam
from scipy.stats import zscore
from bayes_opt import BayesianOptimization
import time
import warnings  # Supress NaN warnings

BASE_PATH = "../../../../local_data/jheaton"
OUTPUT_PATH = os.path.join(BASE_PATH, "class_08_4_bayesian_hyperparameter_opt/")
os.system("mkdir -p " + OUTPUT_PATH)

logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
warnings.filterwarnings("ignore", category=RuntimeWarning)

# Read the data set
df = pd.read_csv(
    "https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv",
    na_values=["NA", "?"],
)
# Generate dummies for job
df = pd.concat([df, pd.get_dummies(df["job"], prefix="job")], axis=1)
df.drop("job", axis=1, inplace=True)
# Generate dummies for area
df = pd.concat([df, pd.get_dummies(df["area"], prefix="area")], axis=1)
df.drop("area", axis=1, inplace=True)
# Missing values for income
med = df["income"].median()
df["income"] = df["income"].fillna(med)
# Standardize ranges
df["income"] = zscore(df["income"])
df["aspect"] = zscore(df["aspect"])
df["save_rate"] = zscore(df["save_rate"])
df["age"] = zscore(df["age"])
df["subscriptions"] = zscore(df["subscriptions"])
# Convert to numpy - Classification
x_columns = df.columns.drop("product").drop("id")
x = df[x_columns].values
dummies = pd.get_dummies(df["product"])  # Classification
products = dummies.columns
y = dummies.values

# print("y\n", y)


def generate_model(dropout, neuronPct, neuronShrink):
    # We start with some percent of 5000 starting neurons on
    # the first hidden layer.
    neuronCount = int(neuronPct * 5000)
    # Construct neural network
    model = Sequential()
    # So long as there would have been at least 25 neurons and
    # fewer than 10
    # layers, create a new layer.
    layer = 0
    while neuronCount > 25 and layer < 10:
        # The first (0th) layer needs an input input_dim(neuronCount)
        if layer == 0:
            model.add(Dense(neuronCount, input_dim=x.shape[1], activation=PReLU()))
        else:
            model.add(Dense(neuronCount, activation=PReLU()))
            layer += 1
        # Add dropout after each hidden layer
        model.add(Dropout(dropout))
        # Shrink neuron count for each layer
        neuronCount = round(neuronCount * neuronShrink)
    model.add(Dense(y.shape[1], activation="softmax"))  # Output
    return model


# Generate a model and see what the resulting structure looks like.
# model = generate_model(dropout=0.2, neuronPct=0.1, neuronShrink=0.25)
# model.summary()


SPLITS = 2
EPOCHS = 500
PATIENCE = 10
call_count = 0


def evaluate_network(dropout, learning_rate, neuronPct, neuronShrink):
    global call_count
    call_count += 1
    # print(f"call {call_count} evaluate_network: dr={dropout:.4f} lr={learning_rate:.4f} nP={neuronPct:.4f} nS={neuronShrink:.4f}")
    print(f"call {call_count} evaluate_network: dr={dropout} lr={learning_rate} nP={neuronPct} nS={neuronShrink}")

    # Bootstrap
    # for Classification
    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)
    # for Regression
    # boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1)
    # Track progress
    mean_benchmark = []
    epochs_needed = []
    num = 0
    # Loop through samples
    for train, test in boot.split(x, df["product"]):
        start_time = time.time()
        num += 1
        # Split train and test
        # x_train = x[train]
        # y_train = y[train]
        # x_test = x[test]
        # y_test = y[test]
        x_train = np.asarray(x[train]).astype(np.float32)
        y_train = np.asarray(y[train]).astype(np.float32)
        x_test = np.asarray(x[test]).astype(np.float32)
        y_test = np.asarray(y[test]).astype(np.float32)

        model = generate_model(dropout, neuronPct, neuronShrink)
        model.summary()
        model.compile(
            loss="categorical_crossentropy", optimizer=Adam(learning_rate=learning_rate)
        )
        monitor = EarlyStopping(
            monitor="val_loss",
            min_delta=1e-3,
            patience=PATIENCE,
            verbose=0,
            mode="auto",
            restore_best_weights=True,
        )
        # Train on the bootstrap sample
        model.fit(
            x_train,
            y_train,
            validation_data=(x_test, y_test),
            callbacks=[monitor],
            verbose=2,
            epochs=EPOCHS,
        )
        epochs = monitor.stopped_epoch
        epochs_needed.append(epochs)
        # Predict on the out of boot (validation)
        pred = model.predict(x_test)
        # Measure this bootstrap's log loss
        y_compare = np.argmax(y_test, axis=1)  # For log loss calculation

        col1 = pd.DataFrame(y_compare, columns=["y_compare"])
        col2 = pd.DataFrame(pred, columns=["p1","p2","p3","p4","p5","p6","p7"])
        compare = pd.concat([col1, col2], axis=1)
        compare.columns = ["y_test", "p1","p2","p3","p4","p5","p6","p7"]
        # print(compare)
        compare.to_csv(OUTPUT_PATH + "df_y_pred_iter"+str(call_count)+".csv", index=False)

        score = metrics.log_loss(y_compare, pred)
        mean_benchmark.append(score)
        m1 = statistics.mean(mean_benchmark)
        m2 = statistics.mean(epochs_needed)
        mdev = statistics.pstdev(mean_benchmark)
        # Record this iteration
        time_took = time.time() - start_time
    tensorflow.keras.backend.clear_session()
    return -m1

# Nicely formatted time string
def hms_string(sec_elapsed):
    h = int(sec_elapsed / (60 * 60))
    m = int((sec_elapsed % (60 * 60)) / 60)
    s = sec_elapsed % 60
    return f"{h}:{m:>02}:{s:>05.2f}"

# print(
#     evaluate_network(dropout=0.2, learning_rate=1e-3, neuronPct=0.2, neuronShrink=0.2)
# )

# evaluate_network(dropout=0.11679448002633344, learning_rate=0.044700185567872745, neuronPct=0.5546086159149624, neuronShrink=0.8547725918091494)
# exit()

# Bounded region of parameter space
pbounds = {
    "dropout": (0.0, 0.499),
    "learning_rate": (0.0, 0.1),
    "neuronPct": (0.01, 1),
    "neuronShrink": (0.01, 0.8),
}
optimizer = BayesianOptimization(
    f=evaluate_network,
    pbounds=pbounds,
    verbose=2,  # verbose = 1 prints only when a maximum
    # is observed, verbose = 0 is silent
    random_state=1,
)
start_time = time.time()
optimizer.maximize(
    init_points=10,
    n_iter=20,
)
time_took = time.time() - start_time
print(f"Total runtime: {hms_string(time_took)}")
print(optimizer.max)
